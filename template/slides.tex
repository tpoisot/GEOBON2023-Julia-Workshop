\documentclass[12pt, smalloffset, compress, aspectratio=1610]{beamer}

\usetheme{pl}

\usepackage[copies]{contour}

\usepackage{longtable}
\usepackage{booktabs}
\usepackage{tabularx}
\usepackage{minted}
\usepackage{listings}
\usepackage{color}
\usepackage{fancyvrb}
\newcommand{\VerbBar}{|}
\newcommand{\VERB}{\Verb[commandchars=\\\{\}]}
\DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\},fontsize=\small}
% Add ',fontsize=\small' for more characters per line
\usepackage[framemethod=tikz]{mdframed}
\definecolor{shadecolor}{HTML}{EEEEEE}
\mdfsetup{
  backgroundcolor=shadecolor,
  linecolor=shadecolor,
  innerleftmargin=5pt,
  innerrightmargin=5pt,
  leftmargin=-5pt,
  rightmargin=-5pt,
  roundcorner=3pt
}
\newenvironment{Shaded}{\begin{mdframed}}{\end{mdframed}}
\newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.26,0.66,0.93}{\textbf{{#1}}}}
\newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.74,0.68,0.62}{\underline{{#1}}}}
\newcommand{\DecValTok}[1]{\textcolor[HTML]{558B2F}{{#1}}}
\newcommand{\BaseNTok}[1]{\textcolor[HTML]{558B2F}{{#1}}}
\newcommand{\FloatTok}[1]{\textcolor[HTML]{558B2F}{{#1}}}
\newcommand{\ConstantTok}[1]{\textcolor[rgb]{0.74,0.68,0.62}{{#1}}}
\newcommand{\CharTok}[1]{\textcolor[HTML]{7E57C2}{{#1}}}
\newcommand{\SpecialCharTok}[1]{\textcolor[HTML]{7E57C2}{{#1}}}
\newcommand{\StringTok}[1]{\textcolor[HTML]{7E57C2}{{#1}}}
\newcommand{\VerbatimStringTok}[1]{\textcolor[HTML]{7E57C2}{{#1}}}
\newcommand{\SpecialStringTok}[1]{\textcolor[HTML]{7E57C2}{{#1}}}
\newcommand{\ImportTok}[1]{\textcolor[rgb]{0.74,0.68,0.62}{{#1}}}
\newcommand{\CommentTok}[1]{\textcolor[HTML]{546E7A}{\textit{{#1}}}}
\newcommand{\DocumentationTok}[1]{\textcolor[HTML]{BCAAA4}{\textit{{#1}}}}
\newcommand{\AnnotationTok}[1]{\textcolor[HTML]{BCAAA4}{\textbf{\textit{{#1}}}}}
\newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.74,0.68,0.62}{{#1}}}
\newcommand{\OtherTok}[1]{\textcolor[rgb]{0.74,0.68,0.62}{{#1}}}
\newcommand{\FunctionTok}[1]{\textcolor[HTML]{26A69A}{\textbf{{#1}}}}
\newcommand{\VariableTok}[1]{\textcolor[rgb]{0.74,0.68,0.62}{{#1}}}
\newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.26,0.66,0.93}{\textbf{{#1}}}}
\newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.74,0.68,0.62}{{#1}}}
\newcommand{\BuiltInTok}[1]{\textcolor[HTML]{42A5F5}{{#1}}}
\newcommand{\ExtensionTok}[1]{\textcolor[rgb]{0.74,0.68,0.62}{{#1}}}
\newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.74,0.68,0.62}{\textbf{{#1}}}}
\newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.74,0.68,0.62}{{#1}}}
\newcommand{\RegionMarkerTok}[1]{\textcolor[rgb]{0.74,0.68,0.62}{{#1}}}
\newcommand{\InformationTok}[1]{\textcolor[rgb]{0.00,0.40,1.00}{\textbf{\textit{{#1}}}}}
\newcommand{\WarningTok}[1]{\textcolor[HTML]{FF6E40}{\textbf{{#1}}}}
\newcommand{\AlertTok}[1]{\textcolor[HTML]{FF3D00}{{#1}}}
\newcommand{\ErrorTok}[1]{\textcolor[HTML]{DD2C00}{\textbf{{#1}}}}
\newcommand{\NormalTok}[1]{\textcolor[HTML]{212121}{{#1}}}

\makeatletter
\newsavebox\pandoc@box
\newcommand*\pandocbounded[1]{% scales image to fit in text height/width
  \sbox\pandoc@box{#1}%
  \Gscale@div\@tempa{0.8\textheight}{\dimexpr\ht\pandoc@box+\dp\pandoc@box\relax}%
  \Gscale@div\@tempb{\linewidth}{\wd\pandoc@box}%
  \ifdim\@tempb\p@<\@tempa\p@\let\@tempa\@tempb\fi% select the smaller of both
  \ifdim\@tempa\p@<\p@\scalebox{\@tempa}{\usebox\pandoc@box}%
  \else\usebox{\pandoc@box}%
  \fi%
}
% Set default figure placement to htbp
\def\fps@figure{htbp}
\makeatother

\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}

\let\OldTexttt\texttt
\renewcommand{\texttt}[1]{\OldTexttt{\color{codecolor}#1}}

\makeatletter
\def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth\else\Gin@nat@width\fi}
\makeatother

\newcommand{\begincols}{\begin{columns}}
\newcommand{\stopcols}{\end{columns}}
\newcommand{\roundpicture}[2]{%
\tikz\node[circle,
          text=white,
          minimum width=4cm,
          minimum height=4cm,
          path picture={
              \node at (path picture bounding box.center){
                  \includegraphics[width=4cm]{#1}
              };
          }]{#2};
}
\newcommand{\plain}[1]{%
\begin{tikzpicture}[remember picture, overlay]
    \node at (current page.north west) [anchor = north west, inner sep = 0pt, outer sep = 0pt] {%
        \pgfuseimage{titlebackground};%
    };

\node at (current page.north west) [anchor = north west, inner sep = 0pt, outer sep = 0pt] {%
                \tikz {\draw [very nearly opaque, backgroundcolor, fill=backgroundcolor] (0,0) rectangle (12,10);}%
            };
            \node at (current page.west) [anchor = west, inner sep = 0cm, outer sep = 0cm, align=left, xshift=1.5cm] {%
                \Large
                #1
            };
        \end{tikzpicture}
}

\title{Interpretable ML for biodiversity}
\subtitle{An introduction using species distribution models}
\date{\today}
\author{Timothée Poisot}
\institute{Université de Montréal}

\begin{document}

\begin{frame}{Main goals}
\phantomsection\label{main-goals}
\begin{enumerate}
\tightlist
\item
  How do we produce a model?
\item
  How do we convey that it works?
\item
  How do we talk about how it makes predictions?
\item
  How do we use it to guide actions?
\end{enumerate}
\end{frame}

\begin{frame}{But why\ldots{}}
\phantomsection\label{but-why}
\begin{description}
\tightlist
\item[\ldots{} think of SDM as ML problems?]
Because they are! We want to learn a predictive algorithm from data
\item[\ldots{} the focus on explainability?]
We cannot ask people to \emph{trust} - we must \emph{convince} and
\emph{explain}
\end{description}
\end{frame}

\begin{frame}{What we will \emph{not} discuss}
\phantomsection\label{what-we-will-not-discuss}
\begin{enumerate}
\tightlist
\item
  Image recognition
\item
  Sound recognition
\item
  Generative AI
\end{enumerate}
\end{frame}

\begin{frame}{Learning/teaching goals}
\phantomsection\label{learningteaching-goals}
\begin{itemize}
\tightlist
\item
  ML basics

  \begin{itemize}
  \tightlist
  \item
    cross-validation
  \item
    hyper-parameters tuning
  \item
    bagging and ensembles
  \end{itemize}
\item
  Pitfalls

  \begin{itemize}
  \tightlist
  \item
    data leakage
  \item
    overfitting
  \end{itemize}
\item
  Explainable ML

  \begin{itemize}
  \tightlist
  \item
    partial responses
  \item
    Shapley values
  \end{itemize}
\item
  Counterfactuals
\end{itemize}
\end{frame}

\section{Problem statement}\label{problem-statement}

\begin{frame}{The problem in ecological terms}
\phantomsection\label{the-problem-in-ecological-terms}
We have information about a species, taking the form of
\((\text{lon}, \text{lat})\) for points where the species was observed

Using this information, we can extract a suite of environmental
variables for the locations where the species was observed

We can do the same thing for locations where the species was not
observed

\alert{Where could we observe this species}?
\end{frame}

\begin{frame}{The problem in ML terms}
\phantomsection\label{the-problem-in-ml-terms}
We have a series of labels \(\mathbf{y}_n \in \mathbb{B}\), and features
\(\mathbf{X}_{m,n} \in \mathbb{R}\)

We want to find an algorithm \(f(\mathbf{x}_m) = \hat y\) that results
in the distance between \(\hat y\) and \(y\) being \emph{small}

An algorithm that does this job well is generalizable (we can apply it
on data it has not been trained on) and makes credible predictions
\end{frame}

\begin{frame}{Setting up the data for our example}
\phantomsection\label{setting-up-the-data-for-our-example}
We will use data on observations of \emph{Turdus torquatus} in
Switzerland, downloaded from the copy of the eBird dataset on GBIF

Two series of environmental layers

\begin{enumerate}
\tightlist
\item
  CHELSA2 BioClim variables (19)
\item
  EarthEnv land cover variables (12)
\end{enumerate}

Now is \emph{not} the time to make assumptions about which are relevant!
\end{frame}

\begin{frame}{The observation data}
\phantomsection\label{the-observation-data}
\pandocbounded{\includegraphics[keepaspectratio]{figures/slides_4_1.png}}~
\end{frame}

\begin{frame}{Problem (and solution)}
\phantomsection\label{problem-and-solution}
We want \(\textbf{y} \in \mathbb{B}\), and so far we are missing
\alert{negative
values}

We generate \alert{pseudo}-absences with the following rules:

\begin{enumerate}
\tightlist
\item
  Locations further away from a presence are more likely
\item
  Locations less than 5km away from a presence are ruled out
\end{enumerate}
\end{frame}

\begin{frame}{The (inflated) observation data}
\phantomsection\label{the-inflated-observation-data}
\pandocbounded{\includegraphics[keepaspectratio]{figures/slides_6_1.png}}~
\end{frame}

\section{Training the model}\label{training-the-model}

\begin{frame}{A simple decision tree}
\phantomsection\label{a-simple-decision-tree}
\end{frame}

\begin{frame}{Setup}
\phantomsection\label{setup}
\end{frame}

\begin{frame}{Cross-validation}
\phantomsection\label{cross-validation}
Can we train the model?

More specifically -- if we train the model, how well can we expect it to
perform?

assumes parallel universes with slightly less data

is the model good?
\end{frame}

\begin{frame}{Null classifiers}
\phantomsection\label{null-classifiers}
coin flip

no skill

constant
\end{frame}

\begin{frame}{Expectations}
\phantomsection\label{expectations}
The null classifiers tell us what we need to beat in order to perform
\alert{better than
random}.

\begin{longtable}[]{@{}rrrrrr@{}}
\toprule\noalign{}
\textbf{Model} & \textbf{MCC} & \textbf{PPV} & \textbf{NPV} &
\textbf{DOR} & \textbf{Accuracy} \\
\midrule\noalign{}
\endhead
No skill & -0.00 & 0.34 & 0.66 & 1.00 & 0.55 \\
Coin flip & -0.32 & 0.34 & 0.34 & 0.26 & 0.34 \\
+ & 0.00 & 0.34 & & & 0.34 \\
- & 0.00 & & 0.66 & & 0.66 \\
\bottomrule\noalign{}
\end{longtable}

In practice, the no-skill classifier is the most informative: what if we
\alert{only} know the positive class prevalence?
\end{frame}

\begin{frame}{Cross-validation strategy}
\phantomsection\label{cross-validation-strategy}
k-fold

validation / training / testing
\end{frame}

\begin{frame}{Cross-validation results}
\phantomsection\label{cross-validation-results}
\begin{longtable}[]{@{}
  >{\raggedleft\arraybackslash}p{(\linewidth - 10\tabcolsep) * \real{0.2647}}
  >{\raggedleft\arraybackslash}p{(\linewidth - 10\tabcolsep) * \real{0.1324}}
  >{\raggedleft\arraybackslash}p{(\linewidth - 10\tabcolsep) * \real{0.1324}}
  >{\raggedleft\arraybackslash}p{(\linewidth - 10\tabcolsep) * \real{0.1324}}
  >{\raggedleft\arraybackslash}p{(\linewidth - 10\tabcolsep) * \real{0.1324}}
  >{\raggedleft\arraybackslash}p{(\linewidth - 10\tabcolsep) * \real{0.2059}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedleft
\textbf{Model}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
\textbf{MCC}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
\textbf{PPV}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
\textbf{NPV}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
\textbf{DOR}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
\textbf{Accuracy}
\end{minipage} \\
\midrule\noalign{}
\endhead
No skill & -0.00 & 0.34 & 0.66 & 1.00 & 0.55 \\
Dec.~tree (val.) & 0.64 & 0.77 & 0.87 & 26.59 & 0.84 \\
Dec.~tree (tr.) & 0.66 & 0.78 & 0.88 & 28.70 & 0.85 \\
\bottomrule\noalign{}
\end{longtable}
\end{frame}

\begin{frame}{What to do if the model is trainable?}
\phantomsection\label{what-to-do-if-the-model-is-trainable}
train it!

re-use the full dataset
\end{frame}

\begin{frame}{The model training pipeline}
\phantomsection\label{the-model-training-pipeline}
\end{frame}

\begin{frame}{Initial prediction}
\phantomsection\label{initial-prediction}
\pandocbounded{\includegraphics[keepaspectratio]{figures/slides_12_1.png}}~
\end{frame}

\begin{frame}{How is this model wrong?}
\phantomsection\label{how-is-this-model-wrong}
\pandocbounded{\includegraphics[keepaspectratio]{figures/slides_13_1.png}}~
\end{frame}

\begin{frame}{Can we improve on this model?}
\phantomsection\label{can-we-improve-on-this-model}
variable selection

data transformation

hyper-parameters tuning

will focus on the later (same process for the two above)
\end{frame}

\begin{frame}{Data leakage}
\phantomsection\label{data-leakage}
\end{frame}

\begin{frame}{A note on PCA}
\phantomsection\label{a-note-on-pca}
\pandocbounded{\includegraphics[keepaspectratio]{figures/slides_15_1.png}}~
\end{frame}

\begin{frame}{Moving theshold classification}
\phantomsection\label{moving-theshold-classification}
p plus \textgreater{} p minus means threshold is 0.5

is it?

how do we check this
\end{frame}

\begin{frame}{Learning curve for the threshold}
\phantomsection\label{learning-curve-for-the-threshold}
\pandocbounded{\includegraphics[keepaspectratio]{figures/slides_17_1.png}}~
\end{frame}

\begin{frame}{Receiver Operating Characteristic}
\phantomsection\label{receiver-operating-characteristic}
\pandocbounded{\includegraphics[keepaspectratio]{figures/slides_18_1.png}}~
\end{frame}

\begin{frame}{Precision-Recall Curve}
\phantomsection\label{precision-recall-curve}
\pandocbounded{\includegraphics[keepaspectratio]{figures/slides_19_1.png}}~
\end{frame}

\begin{frame}{Revisiting the model performance}
\phantomsection\label{revisiting-the-model-performance}
\begin{longtable}[]{@{}
  >{\raggedleft\arraybackslash}p{(\linewidth - 10\tabcolsep) * \real{0.2754}}
  >{\raggedleft\arraybackslash}p{(\linewidth - 10\tabcolsep) * \real{0.1304}}
  >{\raggedleft\arraybackslash}p{(\linewidth - 10\tabcolsep) * \real{0.1304}}
  >{\raggedleft\arraybackslash}p{(\linewidth - 10\tabcolsep) * \real{0.1304}}
  >{\raggedleft\arraybackslash}p{(\linewidth - 10\tabcolsep) * \real{0.1304}}
  >{\raggedleft\arraybackslash}p{(\linewidth - 10\tabcolsep) * \real{0.2029}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedleft
\textbf{Model}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
\textbf{MCC}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
\textbf{PPV}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
\textbf{NPV}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
\textbf{DOR}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
\textbf{Accuracy}
\end{minipage} \\
\midrule\noalign{}
\endhead
No skill & -0.00 & 0.34 & 0.66 & 1.00 & 0.55 \\
Dec.~tree (val.) & 0.64 & 0.77 & 0.87 & 26.59 & 0.84 \\
Dec.~tree (tr.) & 0.66 & 0.78 & 0.88 & 28.70 & 0.85 \\
Tuned tree (val.) & 0.77 & 0.79 & 0.95 & 113.44 & 0.89 \\
Tuned tree (tr.) & 0.80 & 0.81 & 0.96 & 114.37 & 0.90 \\
\bottomrule\noalign{}
\end{longtable}
\end{frame}

\begin{frame}{Updated prediction}
\phantomsection\label{updated-prediction}
\pandocbounded{\includegraphics[keepaspectratio]{figures/slides_22_1.png}}~
\end{frame}

\begin{frame}{How is this model better?}
\phantomsection\label{how-is-this-model-better}
\pandocbounded{\includegraphics[keepaspectratio]{figures/slides_23_1.png}}~
\end{frame}

\begin{frame}{But wait!}
\phantomsection\label{but-wait}
slide on overfitting
\end{frame}

\section{Ensemble models}\label{ensemble-models}

\begin{frame}{Limits of a single model}
\phantomsection\label{limits-of-a-single-model}
\begin{itemize}
\tightlist
\item
  a single model
\item
  different parts of data may have different signal
\item
  do we need all the variables all the time?
\item
  bias v. variance tradeoff
\item
  limit overfitting
\end{itemize}
\end{frame}

\begin{frame}{Bootstrapping and aggregation}
\phantomsection\label{bootstrapping-and-aggregation}
\end{frame}

\begin{frame}{Is this worth it?}
\phantomsection\label{is-this-worth-it}
\begin{longtable}[]{@{}
  >{\raggedleft\arraybackslash}p{(\linewidth - 10\tabcolsep) * \real{0.2754}}
  >{\raggedleft\arraybackslash}p{(\linewidth - 10\tabcolsep) * \real{0.1304}}
  >{\raggedleft\arraybackslash}p{(\linewidth - 10\tabcolsep) * \real{0.1304}}
  >{\raggedleft\arraybackslash}p{(\linewidth - 10\tabcolsep) * \real{0.1304}}
  >{\raggedleft\arraybackslash}p{(\linewidth - 10\tabcolsep) * \real{0.1304}}
  >{\raggedleft\arraybackslash}p{(\linewidth - 10\tabcolsep) * \real{0.2029}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedleft
\textbf{Model}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
\textbf{MCC}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
\textbf{PPV}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
\textbf{NPV}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
\textbf{DOR}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
\textbf{Accuracy}
\end{minipage} \\
\midrule\noalign{}
\endhead
No skill & -0.00 & 0.34 & 0.66 & 1.00 & 0.55 \\
Dec.~tree (val.) & 0.64 & 0.77 & 0.87 & 26.59 & 0.84 \\
Dec.~tree (tr.) & 0.66 & 0.78 & 0.88 & 28.70 & 0.85 \\
Tuned tree (val.) & 0.77 & 0.79 & 0.95 & 113.44 & 0.89 \\
Tuned tree (tr.) & 0.80 & 0.81 & 0.96 & 114.37 & 0.90 \\
Forest (val.) & 0.77 & 0.80 & 0.95 & 109.11 & 0.89 \\
Forest (tr.) & 0.77 & 0.79 & 0.95 & 78.31 & 0.89 \\
\bottomrule\noalign{}
\end{longtable}
\end{frame}

\begin{frame}{Prediction of the rotation forest}
\phantomsection\label{prediction-of-the-rotation-forest}
\pandocbounded{\includegraphics[keepaspectratio]{figures/slides_26_1.png}}~
\end{frame}

\begin{frame}{Prediction of the rotation forest}
\phantomsection\label{prediction-of-the-rotation-forest-1}
\pandocbounded{\includegraphics[keepaspectratio]{figures/slides_27_1.png}}~
\end{frame}

\begin{frame}{Uncertainty}
\phantomsection\label{uncertainty}
\pandocbounded{\includegraphics[keepaspectratio]{figures/slides_28_1.png}}~
\end{frame}

\begin{frame}{Revisiting assumptions}
\phantomsection\label{revisiting-assumptions}
\begin{itemize}
\tightlist
\item
  pseudo-absences
\item
  not just a statistical exercise
\end{itemize}
\end{frame}

\begin{frame}{Variable importance}
\phantomsection\label{variable-importance}
\begin{longtable}[]{@{}rrr@{}}
\toprule\noalign{}
\textbf{Layer} & \textbf{Variable} & \textbf{Import.} \\
\midrule\noalign{}
\endhead
1 & BIO1 & 0.857744 \\
8 & BIO8 & 0.110619 \\
29 & Snow/Ice & 0.0196575 \\
24 & Shrubs & 0.00951455 \\
3 & BIO3 & 0.00246503 \\
\bottomrule\noalign{}
\end{longtable}
\end{frame}

\section{But why?}\label{but-why-1}

\begin{frame}{Intro explainable}
\phantomsection\label{intro-explainable}
\end{frame}

\begin{frame}{Partial response curves}
\phantomsection\label{partial-response-curves}
If we assume that all the variables except one take their average value,
what is the prediction associated to the value that is unchanged?

Equivalent to a mean-field approximation
\end{frame}

\begin{frame}{Example with temperature}
\phantomsection\label{example-with-temperature}
\pandocbounded{\includegraphics[keepaspectratio]{figures/slides_31_1.png}}~
\end{frame}

\begin{frame}{Example with two variables}
\phantomsection\label{example-with-two-variables}
\pandocbounded{\includegraphics[keepaspectratio]{figures/slides_32_1.png}}~
\end{frame}

\begin{frame}{Spatialized partial response plot}
\phantomsection\label{spatialized-partial-response-plot}
\pandocbounded{\includegraphics[keepaspectratio]{figures/slides_33_1.png}}~
\end{frame}

\begin{frame}{Spatialized partial response (binary outcome)}
\phantomsection\label{spatialized-partial-response-binary-outcome}
\pandocbounded{\includegraphics[keepaspectratio]{figures/slides_34_1.png}}~
\end{frame}

\begin{frame}{Inflated response curves}
\phantomsection\label{inflated-response-curves}
Averaging the variables is \alert{masking a lot of variability}!

Alternative solution:

\begin{enumerate}
\tightlist
\item
  Generate a grid for all the variables
\item
  For all combinations in this grid, use it as the stand-in for the
  variables to replace
\end{enumerate}

In practice: Monte-Carlo on a reasonable number of samples.
\end{frame}

\begin{frame}{Example}
\phantomsection\label{example}
\pandocbounded{\includegraphics[keepaspectratio]{figures/slides_36_1.png}}~
\end{frame}

\begin{frame}{Limitations}
\phantomsection\label{limitations}
\begin{itemize}
\tightlist
\item
  partial responses can only generate model-level information
\item
  they break the structure of values for all predictors at the scale of
  a single observation
\item
  their interpretation is unclear
\end{itemize}
\end{frame}

\begin{frame}{Shapley}
\phantomsection\label{shapley}
\end{frame}

\begin{frame}{Example}
\phantomsection\label{example-1}
\end{frame}

\begin{frame}{Response curves revisited}
\phantomsection\label{response-curves-revisited}
\pandocbounded{\includegraphics[keepaspectratio]{figures/slides_38_1.png}}~
\end{frame}

\begin{frame}{On a map}
\phantomsection\label{on-a-map}
\pandocbounded{\includegraphics[keepaspectratio]{figures/slides_39_1.png}}~
\end{frame}

\begin{frame}{Variable importance revisited}
\phantomsection\label{variable-importance-revisited}
\begin{longtable}[]{@{}rrrr@{}}
\toprule\noalign{}
\textbf{Layer} & \textbf{Variable} & \textbf{Import.} & \textbf{Shap.
imp.} \\
\midrule\noalign{}
\endhead
1 & BIO1 & 0.857744 & 0.783837 \\
8 & BIO8 & 0.110619 & 0.140305 \\
29 & Snow/Ice & 0.0196575 & 0.0501887 \\
24 & Shrubs & 0.00951455 & 0.0176572 \\
3 & BIO3 & 0.00246503 & 0.00801191 \\
\bottomrule\noalign{}
\end{longtable}
\end{frame}

\begin{frame}{Most important predictor}
\phantomsection\label{most-important-predictor}
\pandocbounded{\includegraphics[keepaspectratio]{figures/slides_41_1.png}}~
\end{frame}

\begin{frame}{Revisiting the data transformation}
\phantomsection\label{revisiting-the-data-transformation}
all in a single model so we can ask effect of variable instead of effect
of PC1 or whatever
\end{frame}

\section{What if?}\label{what-if}

\begin{frame}{Intro to counterfactuals}
\phantomsection\label{intro-to-counterfactuals}
what they are
\end{frame}

\begin{frame}{The Rashomon effect}
\phantomsection\label{the-rashomon-effect}
\begin{itemize}
\tightlist
\item
  different but equally likely alternatives
\item
  happens at all steps in the process
\item
  variable selected, threshold used, model type
\end{itemize}
\end{frame}

\begin{frame}{Generating a counterfactual}
\phantomsection\label{generating-a-counterfactual}
\end{frame}

\begin{frame}{Evaluating the counterfactuals}
\phantomsection\label{evaluating-the-counterfactuals}
\end{frame}

\begin{frame}{What is a good counterfactual}
\phantomsection\label{what-is-a-good-counterfactual}
learning rate and loss function

use on prediction score and not yes/no!
\end{frame}

\begin{frame}{Algorithmic recourse}
\phantomsection\label{algorithmic-recourse}
\end{frame}

\section{Conclusions}\label{conclusions}

\end{document}
