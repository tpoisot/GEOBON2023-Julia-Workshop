---
title: Interpretable ML for biodiversity
subtitle: An introduction using species distribution models
author: Timothée Poisot
institute: Université de Montréal
date: \today
weave_options:
    doctype: pandoc
    fig_ext: .png
    fig_pos: p
    echo: false
    term: false
    results: hidden
    cache: true
---

```julia
using SpeciesDistributionToolkit
using CairoMakie
using Statistics
using PrettyTables
using Random
using DelimitedFiles
Random.seed!(420)
include("assets/makietheme.jl")
```

## Main goals

1. How do we produce a model?
2. How do we convey that it works?
3. How do we talk about how it makes predictions?
4. How do we use it to guide actions?

## The steps

1. Get data about species occurrences
2. Build a classifier and make it as good as we can
3. Measure its performance
4. Explain some predictions
5. Generate counterfactual explanations
6. Briefly discuss ensemble models

## But why...

... think of SDM as a ML problem?
: Because they are! We want to learn a predictive algorithm from data

... the focus on explainability?
: We cannot ask people to *trust* - we must *convince* and *explain*

# Problem statement

## The problem in ecological terms

We have information about a species

## The problem in other words

We have a series of observations $y \in \mathbb{B}$, and predictors variables
$\mathbf{X} \in \mathbb{R}$

We want to find an algorithm $f(\mathbf{x}) = \hat y$ that results in the
distance between $\hat y$ and $y$ being *small*

## Setting up the data for our example

The predictor data will come from CHELSA2 - we will start with the 19 BioClim
variables

```julia
CHE = SpeciesDistributionToolkit.gadm("CHE");
layernames = readlines("layernames.csv")
predictors = [SDMLayer("layers.tiff"; bandnumber=i) for i in 1:31]
predictors = [mask(p, predictors[end]) for p in predictors]
```

We will use data on observations of *Turdus torquatus* in Switzerland,
downloaded from the copy of the eBird dataset on GBIF

```julia
presences = readdlm("presences.csv")
```

## The observation data

```julia
f = Figure(; size=(800, 400))
ax = Axis(f[1,1], aspect=DataAspect())
scatter!(ax, presences, color=:black)
lines!(ax, CHE.geometry[1]; color = :black)
hidedecorations!(ax)
hidespines!(ax)
current_figure()
```

## Problem!

We want $\hat y \in \mathbb{B}$, and so far we are missing \alert{negative
values}

## Solution!

pseudo-absences

what are the assumptions we make

```julia
presencelayer = zeros(first(predictors), Bool)
for i in axes(presences, 1)
    presencelayer[presences[i,:]...] = true
end
background = pseudoabsencemask(DistanceToEvent, presencelayer)
bgpoints = backgroundpoints(nodata(background, d -> d < 6), 2sum(presencelayer))
prsc = nodata(presencelayer, false)
absc = nodata(bgpoints, false)
```

## The (inflated) observation data

```julia
f = Figure(; size=(800, 400))
ax = Axis(f[1,1], aspect=DataAspect())
scatter!(ax, prsc; color = :black)
scatter!(ax, absc; color = :red, markersize = 4)
lines!(ax, CHE.geometry[1]; color = :black)
hidedecorations!(ax)
hidespines!(ax)
current_figure()
```

# Training the model

## The Naive Bayes Classifier

$$P(+|x) = \frac{P(+)}{P(x)}P(x|+)$$

$$\hat y = \text{argmax}_j \, P(\mathbf{c}_j)\prod_i P(\mathbf{x}_i|\mathbf{c}_j)$$

$$P(x|+) = \text{pdf}(x, \mathcal{N}(\mu_+, \sigma_+))$$

## Setup

```julia
sdm = SDM(MultivariateTransform{PCA}, NaiveBayes, predictors, presencelayer, bgpoints)
```

## Cross-validation

Can we train the model

assumes parallel universes with slightly less data

is the model good?

## Null classifiers

coin flip

no skill

constant

## Expectations

```julia ; results="raw"
hdr = ["Model", "MCC", "PPV", "NPV", "DOR", "Accuracy"]
tbl = []
for null in [noskill, coinflip, constantpositive, constantnegative]
    m = null(sdm)
    push!(tbl, [null, mcc(m), ppv(m), npv(m), dor(m), accuracy(m)])
end
data = permutedims(hcat(tbl...))
pretty_table(data; backend = Val(:markdown), header = hdr)
```

## Cross-validation strategy

k-fold

validation / training / testing

```julia
folds = kfold(sdm);
cv = crossvalidate(sdm, folds; threshold = false);
```

## Cross-validation results

```julia ; results="raw"
hdr = ["Model", "MCC", "PPV", "NPV", "DOR", "Accuracy"]
tbl = []
for null in [noskill, coinflip, constantpositive, constantnegative]
    m = null(sdm)
    push!(tbl, [null, mcc(m), ppv(m), npv(m), dor(m), accuracy(m)])
end
push!(tbl, ["Validation", mean(mcc.(cv.validation)), mean(ppv.(cv.validation)), mean(npv.(cv.validation)), mean(dor.(cv.validation)), mean(accuracy.(cv.validation))])
push!(tbl, ["Training", mean(mcc.(cv.training)), mean(ppv.(cv.training)), mean(npv.(cv.training)), mean(dor.(cv.training)), mean(accuracy.(cv.training))])
data = permutedims(hcat(tbl...))
pretty_table(data; backend = Val(:markdown), header = hdr)
```

## What to do if the model is trainable?

train it!

re-use the full dataset

```julia
train!(sdm; threshold=false)
prd = predict(sdm, predictors; threshold = false)
current_range = predict(sdm, predictors)
```

## A note on data leakage

## Data transformation using PCA

## The model training pipeline

## Initial prediction

```julia
f = Figure(; size = (800, 400))
ax = Axis(f[1, 1]; aspect = DataAspect())
hm = heatmap!(ax, prd; colormap = :linear_worb_100_25_c53_n256, colorrange = (0, 1))
contour!(ax, predict(sdm, predictors); color = :black, linewidth = 0.5)
Colorbar(f[1, 2], hm)
lines!(ax, CHE.geometry[1]; color = :black)
hidedecorations!(ax)
hidespines!(ax)
current_figure()
```

## How is this model wrong?

```julia
f = Figure(; size = (800, 400))
ax = Axis(f[1, 1]; aspect = DataAspect())
heatmap!(ax, current_range, colormap=[colorant"#fefefe", colorant"#d4d4d4"])
scatter!(ax, mask(current_range, prsc) .& prsc; markersize=8, strokecolor=:black, strokewidth=1, color=:transparent, marker=:rect)
scatter!(ax, mask(!current_range, prsc) .& prsc; markersize=8, strokecolor=:red, strokewidth=1, color=:transparent, marker=:rect)
scatter!(ax, mask(current_range, absc) .& absc; markersize=8, strokecolor=:red, strokewidth=1, color=:transparent, marker=:hline)
scatter!(ax, mask(!current_range, absc) .& absc; markersize=8, strokecolor=:black, strokewidth=1, color=:transparent, marker=:hline)
lines!(ax, CHE.geometry[1]; color = :black)
hidedecorations!(ax)
hidespines!(ax)
current_figure()
```

## Can we improve on this model?

variable selection

```julia
forwardselection!(sdm, folds)
```

data transformation

hyper-parameters tuning

will focus on the later (same process for the two above)

## Moving theshold classification

p plus > p minus means threshold is 0.5

is it?

how do we check this

```julia
THR = LinRange(0.0, 1.0, 200)
tcv = [crossvalidate(sdm, folds; thr=thr) for thr in THR]
bst = last(findmax([mean(mcc.(c.training)) for c in tcv]))
```

## Learning curve for the threshold

```julia
f= Figure(; size=(400, 400))
ax = Axis(f[1,1])
lines!(ax, THR, [mean(mcc.(c.validation)) for c in tcv], color=:black)
scatter!(ax, [THR[bst]], [mean(mcc.(tcv[bst].validation))], color=:black)
xlims!(ax, 0., 1.)
ylims!(ax, 0., 1.)
current_figure()
```

## Receiver Operating Characteristic

```julia
f= Figure(; size=(400, 400))
ax = Axis(f[1,1])
lines!(ax, [mean(fpr.(c.validation)) for c in tcv], [mean(tpr.(c.validation)) for c in tcv], color=:black)
scatter!(ax, [mean(fpr.(tcv[bst].validation))], [mean(tpr.(tcv[bst].validation))], color=:black)
xlims!(ax, 0., 1.)
ylims!(ax, 0., 1.)
current_figure()
```

## Precision-Recall Curve

```julia
f= Figure(; size=(400, 400))
ax = Axis(f[1,1])
lines!(ax, [mean(ppv.(c.validation)) for c in tcv], [mean(tpr.(c.validation)) for c in tcv], color=:black)
scatter!(ax, [mean(ppv.(tcv[bst].validation))], [mean(tpr.(tcv[bst].validation))], color=:black)
xlims!(ax, 0., 1.)
ylims!(ax, 0., 1.)
current_figure()
```

## Revisiting the model performance

```julia; results="raw"
cv2 = crossvalidate(sdm, folds; threshold = true)
hdr = ["Model", "MCC", "PPV", "NPV", "DOR", "Accuracy"]
tbl = []
for null in [noskill, coinflip, constantpositive, constantnegative]
    m = null(sdm)
    push!(tbl, [null, mcc(m), ppv(m), npv(m), dor(m), accuracy(m)])
end
push!(tbl, ["Previous", mean(mcc.(cv.validation)), mean(ppv.(cv.validation)), mean(npv.(cv.validation)), mean(dor.(cv.validation)), mean(accuracy.(cv.validation))])
push!(tbl, ["Validation", mean(mcc.(cv2.validation)), mean(ppv.(cv2.validation)), mean(npv.(cv2.validation)), mean(dor.(cv2.validation)), mean(accuracy.(cv2.validation))])
push!(tbl, ["Training", mean(mcc.(cv2.training)), mean(ppv.(cv2.training)), mean(npv.(cv2.training)), mean(dor.(cv2.training)), mean(accuracy.(cv2.training))])
data = permutedims(hcat(tbl...))
pretty_table(data; backend = Val(:markdown), header = hdr)
```

## Updated prediction

```julia
train!(sdm)
prd = predict(sdm, predictors; threshold = false)
current_range = predict(sdm, predictors)
```

```julia
f = Figure(; size = (800, 400))
ax = Axis(f[1, 1]; aspect = DataAspect())
hm = heatmap!(ax, prd; colormap = :linear_worb_100_25_c53_n256, colorrange = (0, 1))
contour!(ax, predict(sdm, predictors); color = :black, linewidth = 0.5)
Colorbar(f[1, 2], hm)
lines!(ax, CHE.geometry[1]; color = :black)
hidedecorations!(ax)
hidespines!(ax)
current_figure()
```

## How is this model better?

```julia
f = Figure(; size = (800, 400))
ax = Axis(f[1, 1]; aspect = DataAspect())
heatmap!(ax, current_range, colormap=[colorant"#fefefeff", colorant"#d4d4d4"])
scatter!(ax, mask(current_range, prsc) .& prsc; markersize=8, strokecolor=:black, strokewidth=1, color=:transparent, marker=:rect)
scatter!(ax, mask(!current_range, prsc) .& prsc; markersize=8, strokecolor=:red, strokewidth=1, color=:transparent, marker=:rect)
scatter!(ax, mask(current_range, absc) .& absc; markersize=8, strokecolor=:red, strokewidth=1, color=:transparent, marker=:hline)
scatter!(ax, mask(!current_range, absc) .& absc; markersize=8, strokecolor=:black, strokewidth=1, color=:transparent, marker=:hline)
lines!(ax, CHE.geometry[1]; color = :black)
hidedecorations!(ax)
hidespines!(ax)
current_figure()
```

## Revisiting assumptions

- pseudo-absences
- not just a statistical exercise

## Variable importance

```julia; results="raw"
var_imp = variableimportance(sdm, folds)
var_imp ./= sum(var_imp)

hdr = ["Layer", "Variable", "Import."]
pretty_table(
    hcat(variables(sdm), layernames[variables(sdm)], var_imp)[sortperm(var_imp; rev=true),:];
    backend = Val(:markdown), header = hdr)
```

# But why?

## Intro explainable

## An ecology tool: partial response curves

## Example with temperature

```julia
x, y = partialresponse(sdm, 24; threshold=false)
f = Figure(; size=(400, 400))
ax = Axis(f[1,1])
lines!(ax, x, y)
current_figure()
```

## Example with two variables

```julia
x, y, z = partialresponse(sdm, 1, 29; threshold=false)
f = Figure(; size=(400, 400))
ax = Axis(f[1,1], xlabel=layernames[1], ylabel=layernames[29])
heatmap!(ax, x, y, z, colormap=:linear_worb_100_25_c53_n256, colorrange=(0,1))
current_figure()
```

## Spatialized partial response plot

```julia
partial_temp = partialresponse(sdm, predictors, 1; threshold=false)
f = Figure(; size = (800, 400))
ax = Axis(f[1, 1]; aspect = DataAspect())
hm = heatmap!(ax, partial_temp; colormap = :linear_wcmr_100_45_c42_n256, colorrange = (0, 1))
contour!(ax, predict(sdm, predictors); color = :black, linewidth = 0.5)
Colorbar(f[1, 2], hm)
lines!(ax, CHE.geometry[1]; color = :black)
hidedecorations!(ax)
hidespines!(ax)
current_figure()
```

## Spatialized partial response (binary outcome)

```julia
partial_temp = partialresponse(sdm, predictors, 1; threshold=true)
f = Figure(; size = (800, 400))
ax = Axis(f[1, 1]; aspect = DataAspect())
hm = heatmap!(ax, partial_temp; colormap = :linear_wcmr_100_45_c42_n256, colorrange = (0, 1))
contour!(ax, predict(sdm, predictors); color = :black, linewidth = 0.5)
Colorbar(f[1, 2], hm)
lines!(ax, CHE.geometry[1]; color = :black)
hidedecorations!(ax)
hidespines!(ax)
current_figure()
```

## Inflated response curves

Averaging the variables is \alert{masking a lot of variability}!

Alternative solution:

1. Generate a grid for all the variables
2. For all combinations in this grid, use it as the stand-in for the variables to replace

In practice: Monte-Carlo on a reasonable number of samples.

## Example

```julia
f = Figure(; size=(400, 400))
ax = Axis(f[1,1])
for i in 1:350
    lines!(ax, partialresponse(sdm, 1; inflated=true, threshold=false)..., color=:lightgrey, alpha=0.5)
end
lines!(ax, partialresponse(sdm, 1; inflated=false, threshold=false)..., color=:black)
ylims!(ax, 0., 1.)
xlims!(ax, extrema(features(sdm, 1))...)
current_figure()
```

## Limitations

- partial responses can only generate model-level information
- they break the structure of values for all predictors at the scale of a single observation
- their interpretation is unclear

## Shapley

## Example

```julia
explain(sdm, 1; threshold=false)
```

## Response curves revisited

```julia
f = Figure(; size=(800, 400))
ax = Axis(f[1,1])
hexbin!(ax, features(sdm, 1), explain(sdm, 1; threshold=false), bins=60, colormap=:linear_bgyw_15_100_c68_n256)
ax2 = Axis(f[1,2])
hist!(ax2, explain(sdm, 1; threshold=false), color=:lightgrey, strokecolor=:black, strokewidth=1)
current_figure()
```

## On a map

```julia
shapley_temp = explain(sdm, predictors, 1; threshold=false)
f = Figure(; size = (800, 400))
ax = Axis(f[1, 1]; aspect = DataAspect())
hm = heatmap!(ax, shapley_temp; colormap = :diverging_bwg_20_95_c41_n256, colorrange = (-0.4, 0.4))
contour!(ax, predict(sdm, predictors); color = :black, linewidth = 0.5)
Colorbar(f[1, 2], hm)
lines!(ax, CHE.geometry[1]; color = :black)
hidedecorations!(ax)
hidespines!(ax)
current_figure()
```

## Variable importance revisited

```julia; results="raw"
S = [explain(sdm, predictors, v; threshold=false) for v in variables(sdm)]
shap_imp = map(x -> sum(abs.(x)), S)
shap_imp ./= sum(shap_imp)
most_imp = mosaic(x -> argmax(abs.(x)), S)
hdr = ["Layer", "Variable", "Import.", "Shap. imp."]
pretty_table(
    hcat(variables(sdm), layernames[variables(sdm)], var_imp, shap_imp)[sortperm(shap_imp; rev=true),:];
    backend = Val(:markdown), header = hdr)
```

## Most important predictor

```julia
f = Figure(; size = (800, 400))
ax = Axis(f[1, 1]; aspect = DataAspect())
var_colors = cgrad(:diverging_rainbow_bgymr_45_85_c67_n256, length(variables(sdm)), categorical=true)
hm = heatmap!(ax, most_imp; colormap = var_colors, colorrange=(1, length(variables(sdm))))
contour!(ax, predict(sdm, predictors); color = :black, linewidth = 0.5)
lines!(ax, CHE.geometry[1]; color = :black)
hidedecorations!(ax)
hidespines!(ax)
Legend(
    f[2, 1],
    [PolyElement(; color = var_colors[i]) for i in 1:length(variables(sdm))],
    layernames[variables(sdm)];
    orientation = :horizontal,
    nbanks = 1,
)
current_figure()
```

## Revisiting the data transformation

all in a single model so we can ask effect of variable instead of effect of PC1 or whatever

# What if?

## Intro to counterfactuals

what they are

## Setting up a new problem

- land use
- decision tree - very easy to overfit
- at most 18 nodes of depth at most 7
- same process

```julia
tree = SDM(MultivariateTransform{PCA}, DecisionTree, predictors, presencelayer, bgpoints)
maxnodes!(tree, 20)
maxdepth!(tree, 6)
variables!(tree, variables(sdm))
train!(tree)
```

## Variable importance

```julia; results="raw"
lu_var_imp = variableimportance(tree, folds; threshold=false)
lu_var_imp ./= sum(lu_var_imp)
hdr = ["Layer", "Variable", "Relative importance"]
tbl = []
for i in eachindex(lu_var_imp)
    push!(tbl, [variables(tree)[i], layernames[variables(tree)[i]], lu_var_imp[i]])
end
pretty_table(
    permutedims(hcat(tbl...))[sortperm(lu_var_imp; rev=true), :];
    backend = Val(:markdown), header = hdr)
```

## Visualizing the prediction

```julia
f = Figure(; size = (800, 400))
ax = Axis(f[1, 1]; aspect = DataAspect())
hm = heatmap!(ax, predict(tree, predictors; threshold=false); colormap = :linear_worb_100_25_c53_n256, colorrange = (0, 1))
contour!(ax, predict(tree, predictors); color = :black, linewidth = 0.5)
Colorbar(f[1, 2], hm)
lines!(ax, CHE.geometry[1]; color = :black)
hidedecorations!(ax)
hidespines!(ax)
current_figure()
```

## The Rashomon effect

- different but equally likely alternatives
- happens at all steps in the process
- variable selected, threshold used, model type

## Visualizing the errors

```julia
current_range = predict(tree, predictors)
f = Figure(; size = (800, 400))
ax = Axis(f[1, 1]; aspect = DataAspect())
poly!(ax, CHE.geometry[1], color=:lightgrey)
heatmap!(ax, current_range, colormap=[colorant"#fefefe", colorant"#d4d4d4"])
scatter!(ax, mask(current_range, prsc) .& prsc; markersize=8, strokecolor=:black, strokewidth=1, color=:transparent, marker=:rect)
scatter!(ax, mask(!current_range, prsc) .& prsc; markersize=8, strokecolor=:red, strokewidth=1, color=:transparent, marker=:rect)
scatter!(ax, mask(current_range, absc) .& absc; markersize=8, strokecolor=:red, strokewidth=1, color=:transparent, marker=:hline)
scatter!(ax, mask(!current_range, absc) .& absc; markersize=8, strokecolor=:black, strokewidth=1, color=:transparent, marker=:hline)
lines!(ax, CHE.geometry[1]; color = :black)
hidedecorations!(ax)
hidespines!(ax)
current_figure()
```

## Partial response (Shapley)

```julia
ft1 = features(tree, 1)
sp1 = explain(tree, 1; threshold=false, samples=100)
f = Figure(; size=(400, 400))
ax = Axis(f[1,1]; xlabel=layernames[variables(tree)[1]])
scatter!(ax, ft1, sp1, color=:black)
current_figure()
```

## Generating a counterfactual

## Evaluating the counterfactuals

## What is a good counterfactual

learning rate and loss function

use on prediction score and not yes/no!

## Algorithmic recourse

# Ensemble models

## Limits of a single model

- a single model
- different parts of data may have different signal
- do we need all the variables all the time?
- bias v. variance tradeoff
- limit overfitting

## Bootstrapping and aggregation

## An example of bagging: rotation forest

```julia
forest = Bagging(tree, 128)
for arbre in forest.models
    variables!(arbre, unique(rand(variables(forest), length(variables(forest)))))
end
train!(forest)
```

## Prediction of the rotation forest

```julia
f = Figure(; size = (800, 400))
ax = Axis(f[1, 1]; aspect = DataAspect())
hm = heatmap!(ax, predict(forest, predictors; threshold=false); colormap = :linear_worb_100_25_c53_n256, colorrange = (0, 1))
contour!(ax, predict(forest, predictors; consensus=majority); color = :black, linewidth = 0.5)
Colorbar(f[1, 2], hm)
lines!(ax, CHE.geometry[1]; color = :black)
hidedecorations!(ax)
hidespines!(ax)
current_figure()
```

## Prediction of the rotation forest

```julia
current_range = predict(forest, predictors; consensus=majority)
f = Figure(; size = (800, 400))
ax = Axis(f[1, 1]; aspect = DataAspect())
poly!(ax, CHE.geometry[1], color=:lightgrey)
heatmap!(ax, current_range, colormap=[colorant"#fefefe", colorant"#d4d4d4"])
scatter!(ax, mask(current_range, prsc) .& prsc; markersize=8, strokecolor=:black, strokewidth=1, color=:transparent, marker=:rect)
scatter!(ax, mask(!current_range, prsc) .& prsc; markersize=8, strokecolor=:red, strokewidth=1, color=:transparent, marker=:rect)
scatter!(ax, mask(current_range, absc) .& absc; markersize=8, strokecolor=:red, strokewidth=1, color=:transparent, marker=:hline)
scatter!(ax, mask(!current_range, absc) .& absc; markersize=8, strokecolor=:black, strokewidth=1, color=:transparent, marker=:hline)
lines!(ax, CHE.geometry[1]; color = :black)
hidedecorations!(ax)
hidespines!(ax)
current_figure()
```

## Uncertainty

```julia
f = Figure(; size = (800, 400))
ax = Axis(f[1, 1]; aspect = DataAspect())
hm = heatmap!(ax, predict(forest, predictors; consensus=iqr, threshold=false); colormap = :linear_wyor_100_45_c55_n256, colorrange = (0, 1))
contour!(ax, predict(forest, predictors; consensus=majority); color = :black, linewidth = 0.5)
Colorbar(f[1, 2], hm)
lines!(ax, CHE.geometry[1]; color = :black)
hidedecorations!(ax)
hidespines!(ax)
current_figure()
```

## Heterogeneous ensembles

## Setting up an heterogeneous ensemble

# Conclusions
